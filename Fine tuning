from tensorflow.keras.models import load_model

# Load the saved model
model = load_model("LSTM.keras")
# Check trainable parameters for each layer
for layer in model.layers:
    print(f"Layer: {layer.name}, Trainable parameters: {layer.count_params()}")
import numpy as np
from keras.utils import to_categorical
from keras.models import Sequential, load_model, Model
from keras.layers import LSTM, Dense, Embedding, Input
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from rdkit import Chem
import pandas as pd

# Load your smaller dataset
small_df = pd.read_csv(small_dataset_path)
# Assuming the SMILES strings are in a column named 'smiles' in your dataset
raw_text = "\n".join(DF['SMILES'].str.strip())

# Creating mapping for each character to integer
unique_chars = sorted(list(set(raw_text)))

# Maps each unique character as an int
char_to_int = dict((c, i) for i, c in enumerate(unique_chars))
# Manually updates \n
char_to_int.update({-1: "\n"})

# Int to char dictionary
int_to_char = dict((i, c) for i, c in enumerate(unique_chars))
int_to_char.update({"\\n": -1})

# Summarize the loaded data to provide lengths for preparing datasets
n_chars = len(raw_text)
n_vocab = len(unique_chars)

print("Total number of characters in the file is:", n_chars)
print("Total number of unique characters in the file is:", n_vocab)

# Calculate average sequence length from the dataset
avg_seq_length = sum(len(smiles) for smiles in DF['SMILES']) // len(DF)
print("Average sequence length in the dataset is:", avg_seq_length)

# Define sequence length
seq_length = avg_seq_length

# Preparing datasets by matching the dataset lengths
dataX = []
dataY = []

for i in range(0, n_chars - seq_length, 1):
    seq_in = raw_text[i:i + seq_length]
    seq_out = raw_text[i + seq_length]
    dataX.append([char_to_int[char] for char in seq_in])
    dataY.append(char_to_int[seq_out])

n_patterns = len(dataX)
print("Total number of patterns (sequences) is:", n_patterns)

# Reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (n_patterns, seq_length, 1))
# Normalize data
X = X / float(n_vocab)

# One-hot encode the output variable
y = to_categorical(dataY)


from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense, Embedding
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load the pre-trained model
pretrained_model = load_model("LSTM")

# Freeze the layers of the pre-trained model
for layer in pretrained_model.layers:
    layer.trainable = False
    
for layer in pretrained_model.layers[:4]:
    layer.trainable = False

for layer in pretrained_model.layers[4:]:
    layer.trainable = True

fine_tuned_model = Sequential()

# Reuse pretrained layers
for layer in pretrained_model.layers:
    fine_tuned_model.add(layer)

# Remove old output layer
fine_tuned_model.pop()

# Add new output layer
fine_tuned_model.add(Dense(n_vocab, activation="softmax"))
# Compile the fine-tuned model
fine_tuned_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the fine-tuned model
checkpoint = ModelCheckpoint("LSTM_finetuned", save_best_only=True)
fine_tuned_model.fit(X, y, epochs, batch_size, callbacks=[checkpoint])

# Save the fine-tuned model
fine_tuned_model.save("LSTM_finetuned")

